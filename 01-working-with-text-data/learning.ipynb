{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 1: Working with Text data\n",
    "\n",
    "## Unstructured data and vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folks apply machine learning to text for a number of different applications. Sentiment analysis, when you try to infer the mood of a sentence, is heavily used in marketing. Your Gmail's spam filter is working hard to protect you from all that evil stuff. Language identification, machine translation and topic detection are just a couple of other examples of how data science is applied to text data nowadays.\n",
    "\n",
    "Many times machine learning algorithms are limited by the amount of data available for a given task. This is not the case for text, right? There is plenty of text data out there, especially since the Web shaped our lifes. Yet, machine learning struggled to provide near-humans results on text data for a long time. Why is that?\n",
    "\n",
    "In machine learning, we describe examples, or data points, by a collection of certain features. A person can be described in terms of age, gender and profession, a song has name, album, gender, and so on, as many of the datasets you have seen so far. When data has this structure, we call it **structured data**.\n",
    "\n",
    "In contract, text has no \"natural\" features. It's a sequence of signs that has somehow a meaning. Like images and sound, it is an example of **unstructured data**. While it's pretty easy for us to understand, machines find unstructured data surprisindly hard to process (*dumb machines, one point for us*). \n",
    "\n",
    "For this reason, we can not feed a machine learning classifier directly with text. We need to find a way to *make it structured*. When it comes to unstructured data, most approaches in machine learning solves the problem by finding a (good) way to squeeze the data into some form of vector space. Text is not exception.\n",
    "\n",
    "This process, called **vectorization**, is key to obtaining good results when applying machine learning to text data. Much research has been done in this area and there are classical vectorization approaches. More recently, deep learning revolutioned this field (among others) providing very effective way to vectorize text. We treat some of the typical approaches in this unit, but `[SPOILER ALERT]` you will deal with some cool deep learning stuff in the last learning unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so, vectorization. Let's extract interesting features from text. How do we transform a document into a vector?\n",
    "\n",
    "Firstly, a vector exists in a space. Our space will be an high-dimensional space corresponding to all the **keywords**. So, as a first attempt to build this space, we take each term appearing in the documents as a dimension, or a feature.\n",
    "\n",
    "Some terminology: the collection of all the documents is the *corpus*. We call the *dictionary* the collection of distinct words occurring in the corpus.\n",
    "\n",
    "![vector space](files/vector_space1.png)\n",
    "*Note: due to our physical world unfortunate limitations, we can only draw examples in 3 dimensions. Imagine the same picture with thousands of dimensions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I know, time to see some code. \n",
    "\n",
    "Let's use the following text as a toy example. Those are examples of news headline from the [News Aggregator dataset](https://archive.ics.uci.edu/ml/datasets/News+Aggregator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "health_document ='Men With Prostate Cancer May Not Always Get Better' # better start prevention early, guys\n",
    "tech_document = \"Facebook is always trying to alter people's behavior, says former data scientist. Facebook's Data Science team may have run hundreds of experiments without people's knowledge.\"\n",
    "tv_document = \"Game of Thrones Season 4 Spoilers, Rumors and News: Trailer Video is Sparking Questions, 'Who Will Get Vengeance And Who will Not?'\"\n",
    "docs = [health_document, tech_document, tv_document]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to build our dictionary. As we saw a bit above, it is the collection of all the keywords in the corpus. \n",
    "Again, as a first attempt, let's pretend all words are keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'Who\",\n",
       " '4',\n",
       " 'Always',\n",
       " 'And',\n",
       " 'Better',\n",
       " 'Cancer',\n",
       " 'Data',\n",
       " 'Facebook',\n",
       " \"Facebook's\",\n",
       " 'Game',\n",
       " 'Get',\n",
       " 'May',\n",
       " 'Men',\n",
       " 'News:',\n",
       " 'Not',\n",
       " \"Not?'\",\n",
       " 'Prostate',\n",
       " 'Questions,',\n",
       " 'Rumors',\n",
       " 'Science',\n",
       " 'Season',\n",
       " 'Sparking',\n",
       " 'Spoilers,',\n",
       " 'Thrones',\n",
       " 'Trailer',\n",
       " 'Vengeance',\n",
       " 'Video',\n",
       " 'Who',\n",
       " 'Will',\n",
       " 'With',\n",
       " 'alter',\n",
       " 'always',\n",
       " 'and',\n",
       " 'behavior,',\n",
       " 'data',\n",
       " 'experiments',\n",
       " 'former',\n",
       " 'have',\n",
       " 'hundreds',\n",
       " 'is',\n",
       " 'knowledge.',\n",
       " 'may',\n",
       " 'of',\n",
       " \"people's\",\n",
       " 'run',\n",
       " 'says',\n",
       " 'scientist.',\n",
       " 'team',\n",
       " 'to',\n",
       " 'trying',\n",
       " 'will',\n",
       " 'without'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_dictionary():\n",
    "    dictionary = set()\n",
    "\n",
    "    for doc in docs:\n",
    "        words = doc.split()\n",
    "        dictionary.update(words)\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "build_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how Python's `set` object takes care of removing duplicates for words that appear more than once, like *\"not\"* and *\"is\"*. \n",
    "\n",
    "So, we have our dictionary. Those words are the dimensions of the feature space. \n",
    "We will transform the sentences to vectors (jargon: `project the sentences onto the vector space`) using a disturbingly simple approach: each feature, which correspond to a word, is equal to the count of the times the word appears in the document.\n",
    "\n",
    "Easier to code than to say:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 1, 0, 0, 2, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 2, 0, 0, 0, 1, 0,\n",
       "        1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 1,\n",
       "        1, 0, 0, 0, 0, 0]),\n",
       " array([1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 1, 0, 2, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 1, 1, 1, 1])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorize():\n",
    "    dictionary = build_dictionary()\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        words = doc.split(' ')\n",
    "        vector = np.array([doc.count(word) for word in dictionary])\n",
    "        vectors.append(vector)\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "vectorize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Congratulations, you have three vectors of integers representing your documents! \n",
    "\n",
    "Not very informative, though. It's just a bunch of numbers. Pandas can help us out here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Game</th>\n",
       "      <th>Trailer</th>\n",
       "      <th>to</th>\n",
       "      <th>With</th>\n",
       "      <th>Not</th>\n",
       "      <th>people's</th>\n",
       "      <th>Facebook's</th>\n",
       "      <th>may</th>\n",
       "      <th>May</th>\n",
       "      <th>Season</th>\n",
       "      <th>...</th>\n",
       "      <th>of</th>\n",
       "      <th>Science</th>\n",
       "      <th>is</th>\n",
       "      <th>data</th>\n",
       "      <th>have</th>\n",
       "      <th>Thrones</th>\n",
       "      <th>News:</th>\n",
       "      <th>4</th>\n",
       "      <th>'Who</th>\n",
       "      <th>Will</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Game  Trailer  to  With  Not  people's  Facebook's  may  May  Season  ...   \\\n",
       "0     0        0   0     1    1         0           0    0    1       0  ...    \n",
       "1     0        0   1     0    0         2           1    1    0       0  ...    \n",
       "2     1        1   0     0    1         0           0    0    0       1  ...    \n",
       "\n",
       "   of  Science  is  data  have  Thrones  News:  4  'Who  Will  \n",
       "0   0        0   0     0     0        0      0  0     0     0  \n",
       "1   1        1   2     1     1        0      0  0     0     0  \n",
       "2   1        0   1     0     0        1      1  1     1     1  \n",
       "\n",
       "[3 rows x 52 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_df():\n",
    "    return pd.DataFrame(vectorize(), columns=build_dictionary())\n",
    "\n",
    "build_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, much better. This DataFrame is way more similar to structured data than the initial documents. Still, there are a couple of things we can improve here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The dictionary contains both *Always* and *always*, some punctuation and even the number 4. Ugly, isn't it? A smart and easy improvement would be to convert all words to lowercase and strip numbers and punctuation signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>to</th>\n",
       "      <th>may</th>\n",
       "      <th>spoilers</th>\n",
       "      <th>science</th>\n",
       "      <th>hundreds</th>\n",
       "      <th>experiments</th>\n",
       "      <th>men</th>\n",
       "      <th>sparking</th>\n",
       "      <th>and</th>\n",
       "      <th>...</th>\n",
       "      <th>scientist</th>\n",
       "      <th>get</th>\n",
       "      <th>vengeance</th>\n",
       "      <th>is</th>\n",
       "      <th>with</th>\n",
       "      <th>data</th>\n",
       "      <th>peoples</th>\n",
       "      <th>behavior</th>\n",
       "      <th>have</th>\n",
       "      <th>trailer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   news  to  may  spoilers  science  hundreds  experiments  men  sparking  \\\n",
       "0     0   0    1         0        0         0            0    1         0   \n",
       "1     0   1    1         0        1         1            1    1         0   \n",
       "2     1   0    0         1        0         0            0    0         1   \n",
       "\n",
       "   and   ...     scientist  get  vengeance  is  with  data  peoples  behavior  \\\n",
       "0    0   ...             0    1          0   0     1     0        0         0   \n",
       "1    0   ...             1    0          0   2     1     2        2         1   \n",
       "2    2   ...             0    1          1   1     0     0        0         0   \n",
       "\n",
       "   have  trailer  \n",
       "0     0        0  \n",
       "1     1        0  \n",
       "2     0        1  \n",
       "\n",
       "[3 rows x 44 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "docs = [re.sub(r'[\\d+'+string.punctuation+']', '', doc.lower()) for doc in docs]\n",
    "\n",
    "build_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much cleaner, plus the space now is 44-dimensional, instead of 52-dimensional. Reducing dimensionality of data is always a good thing.\n",
    "\n",
    "Taking a closer look to the DataFrame, it looks like most words just belong to one document. It would be a good signal, because they'd be very discriminative feature (*ok, at least in this toy example*). But there are other words that appear in more than one documents and aren't very informative: `to`, `not`, `is`, `with`.\n",
    "\n",
    "Those words are called **stop words**. They are the most common words in a language, thus they usually appear in almost every documents of a corpus. Not adding much information, right? Let's strip them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>may</th>\n",
       "      <th>spoilers</th>\n",
       "      <th>science</th>\n",
       "      <th>hundreds</th>\n",
       "      <th>experiments</th>\n",
       "      <th>men</th>\n",
       "      <th>sparking</th>\n",
       "      <th>trying</th>\n",
       "      <th>run</th>\n",
       "      <th>...</th>\n",
       "      <th>without</th>\n",
       "      <th>game</th>\n",
       "      <th>rumors</th>\n",
       "      <th>scientist</th>\n",
       "      <th>get</th>\n",
       "      <th>vengeance</th>\n",
       "      <th>data</th>\n",
       "      <th>peoples</th>\n",
       "      <th>behavior</th>\n",
       "      <th>trailer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   news  may  spoilers  science  hundreds  experiments  men  sparking  trying  \\\n",
       "0     0    1         0        0         0            0    1         0       0   \n",
       "1     0    1         0        1         1            1    1         0       1   \n",
       "2     1    0         1        0         0            0    0         1       0   \n",
       "\n",
       "   run   ...     without  game  rumors  scientist  get  vengeance  data  \\\n",
       "0    0   ...           0     0       0          0    1          0     0   \n",
       "1    1   ...           1     0       0          1    0          0     2   \n",
       "2    0   ...           0     1       1          0    1          1     0   \n",
       "\n",
       "   peoples  behavior  trailer  \n",
       "0        0         0        0  \n",
       "1        2         1        0  \n",
       "2        0         0        1  \n",
       "\n",
       "[3 rows x 35 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The list came from here: http://snowball.tartarus.org/algorithms/english/stop.txt\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n",
    "\n",
    "def build_dictionary():\n",
    "    dictionary = set()\n",
    "\n",
    "    for doc in docs:\n",
    "        words = [word for word in doc.split() if word not in stop_words]\n",
    "        dictionary.update(words)\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "def vectorize():\n",
    "    dictionary = build_dictionary()\n",
    "    vectors = []\n",
    "    for doc in docs:\n",
    "        words = doc.split(' ')\n",
    "        vector = np.array([doc.count(word) for word in dictionary if word not in stop_words])\n",
    "        vectors.append(vector)\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "build_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When we compute the vectors just based on the counts of words inside documents, there is a subtle problem. Let's compute the length (AKA norm) of the vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     7\n",
       "1    23\n",
       "2    12\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = build_df()\n",
    "df.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second vector has a **much** bigger norm than the other two. It's because the document contains more words. This may lead to misclassification due to the document length, and we don't want that.\n",
    "\n",
    "Better normalize our vectors, dividing the counts for the document length. This representation is commonly referred as **Term Frequency (TF)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf = df.div(df.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now vectors' norms are all equals and unitary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\n",
       "1    1.0\n",
       "2    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better for our classifier, but we are not done yet.\n",
    "\n",
    "Not all the words has the same importance to find out what is the topic of a document. If a document is about technology, a word like _\"data\"_ is much more informative than _\"always\"_, and it should weight more toward a decision. In general, words that are very common in the corpus are less informative than rare words.\n",
    "\n",
    "That is the rational behind **Inverse Document Frequency (IDF)**:\n",
    "$$ IDF _{term} = log {\\frac{\\lvert D \\lvert}{\\lvert D_{term} \\lvert}}   $$\n",
    "\n",
    "where $D$ is the corpus, while $D_{term}$ is the subset of $D$ that contains $term$.\n",
    "\n",
    "Combining TF and IDF, we define the values of our vectors as:\n",
    "\n",
    "$$ TFIDF _{term} = TF _{term} * IDF _{term} $$\n",
    "\n",
    "In short, we measure *the term frequency, weighted by its rarity in the entire corpus*, as perfectly put by Maria Dominguez during the 4th hackaton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>may</th>\n",
       "      <th>spoilers</th>\n",
       "      <th>science</th>\n",
       "      <th>hundreds</th>\n",
       "      <th>experiments</th>\n",
       "      <th>men</th>\n",
       "      <th>sparking</th>\n",
       "      <th>trying</th>\n",
       "      <th>run</th>\n",
       "      <th>...</th>\n",
       "      <th>without</th>\n",
       "      <th>game</th>\n",
       "      <th>rumors</th>\n",
       "      <th>scientist</th>\n",
       "      <th>get</th>\n",
       "      <th>vengeance</th>\n",
       "      <th>data</th>\n",
       "      <th>peoples</th>\n",
       "      <th>behavior</th>\n",
       "      <th>trailer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047766</td>\n",
       "      <td>0.047766</td>\n",
       "      <td>0.047766</td>\n",
       "      <td>0.017629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047766</td>\n",
       "      <td>0.047766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095532</td>\n",
       "      <td>0.095532</td>\n",
       "      <td>0.047766</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.091551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091551</td>\n",
       "      <td>0.091551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033789</td>\n",
       "      <td>0.091551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       news       may  spoilers   science  hundreds  experiments       men  \\\n",
       "0  0.000000  0.057924  0.000000  0.000000  0.000000     0.000000  0.057924   \n",
       "1  0.000000  0.017629  0.000000  0.047766  0.047766     0.047766  0.017629   \n",
       "2  0.091551  0.000000  0.091551  0.000000  0.000000     0.000000  0.000000   \n",
       "\n",
       "   sparking    trying       run    ...      without      game    rumors  \\\n",
       "0  0.000000  0.000000  0.000000    ...     0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.047766  0.047766    ...     0.047766  0.000000  0.000000   \n",
       "2  0.091551  0.000000  0.000000    ...     0.000000  0.091551  0.091551   \n",
       "\n",
       "   scientist       get  vengeance      data   peoples  behavior   trailer  \n",
       "0   0.000000  0.057924   0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1   0.047766  0.000000   0.000000  0.095532  0.095532  0.047766  0.000000  \n",
       "2   0.000000  0.033789   0.091551  0.000000  0.000000  0.000000  0.091551  \n",
       "\n",
       "[3 rows x 35 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def idf(column):\n",
    "    return np.log(len(column) / sum(column > 0))\n",
    "\n",
    "tf_idf = tf.multiply(tf.apply(idf))\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have obtained a satisfying feature representation, it's probably time to feed a classifier with those features. Traditionally, good choices to classify text are Support Vector Machines and Bayesian classifiers. Due to its easy analytical tractability, let's choose a Naive Bayes classifier and see in details how it works.\n",
    "\n",
    "Given $k$ classes $\\{C_{1}, ..., C_{k}\\}$ and a document $D$ to classify, our goal is to estimate the probabilistic model $P(C_{i} \\vert D)$.\n",
    "\n",
    "At classification time, we will choose the *maximum a posteriori*, or, in plain English, the most likely class. Computing that is simple: it will be the class $C^{*}$ such that:\n",
    "\n",
    "$$C^{*} = argmax_{C_{i}} P(C_{i} \\vert D)$$ \n",
    "\n",
    "There is still the issue to compute $P(C_{i} \\vert D)$. Naive Bayes, as other Bayesian classifiers, uses Bayes' theorem to approach the problem: \n",
    "\n",
    "$$P(C_{i} \\vert D) = \\frac{P(D \\lvert C_{i}) P(C_{i})}{P(D)}$$\n",
    "\n",
    "Huum, does not look like a great step ahead. But let's have a deeper look to the formula.\n",
    "\n",
    "The denominator $P(D)$ is the **evidence**. We can consider it constant, because it simply is our document, no uncertainty involved. Ok, let's forget about it.\n",
    "$P(C_{i})$ is called the **prior**, and it does not depend on $D$. It can be treated as the probability of assigning class $C_{i}$ without even looking at the document. We can disregard it, right? \n",
    "\n",
    "We are left with the much more interesting $P(D \\lvert C_{i})$, which is called the **likelihood**. In fact, it is the likelihood of the document $D$ to belong to class $C_{i}$. \n",
    "\n",
    "Remembering that we described a document in terms of its keywords $w_{j}$, this probability is equal to:\n",
    "\n",
    "$$P(D \\lvert C_{i}) = P(w_{1} \\vert C_{i}, w_{2} \\vert C_{i}, \\dots,  w_{n} \\vert C_{i})$$\n",
    "\n",
    "Intuitively, you can imagine the classifier asking itself: *How is it likely that those words appear in a document about health?*, *How is it likely that those words appear in a document about technology?*, and so on.\n",
    "\n",
    "Now, it is time for the **Naive assumptions**: each word $w_{j}$ is independent of the presence and the order of every other words. With these assumptions, the formula above is much more easily computable:\n",
    "\n",
    "$$P(D \\lvert C_{i}) = \\prod_{j=1}^N P(w_{j} \\lvert C_{i})$$\n",
    "\n",
    "In theory, computing $P(w_{j} \\lvert C_{i})$ is as easy as counting the occurrences of the word $w_{j}$ in documents of class $C_{i}$. In practice, it may produce extremely small numbers, though, leading to wrong results. An approximation that solves this problem is as follows:\n",
    "\n",
    "$$P(w_{j} \\lvert C_{i}) = \\frac {occurrences \\, of \\, w_{j} \\, in \\, documents \\, of \\, C_{i} + 1} {total \\, words \\, in \\, C_{i} + \\lvert vocabulary \\lvert} $$\n",
    "\n",
    "That is easy to translate into Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prob_word_given_class(word, C, train_set):\n",
    "    word_in_class = sum(train_set.loc[train_set.target == C, word])\n",
    "    total_words_in_class = sum((train_set[train_set.target == C] != 0).any(axis=1))\n",
    "    dictionary_cardinality = train_set.shape[1]\n",
    "    \n",
    "    return (1 + word_in_class) / (total_words_in_class + dictionary_cardinality)\n",
    "\n",
    "def prob_document_of_class(doc, C, train_set):\n",
    "    preprocessed = re.sub(r'[\\d+'+string.punctuation+']', '', doc.lower())\n",
    "    words = pd.Series([word for word in preprocessed.split() if word in train_set.columns])\n",
    "    probs = words.apply(lambda w: prob_word_given_class(w, C, train_set))\n",
    "    return probs.prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all well and good, but we should apply all this math!\n",
    "\n",
    "First things first. We turn our features into a real training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news</th>\n",
       "      <th>may</th>\n",
       "      <th>spoilers</th>\n",
       "      <th>science</th>\n",
       "      <th>hundreds</th>\n",
       "      <th>experiments</th>\n",
       "      <th>men</th>\n",
       "      <th>sparking</th>\n",
       "      <th>trying</th>\n",
       "      <th>run</th>\n",
       "      <th>...</th>\n",
       "      <th>game</th>\n",
       "      <th>rumors</th>\n",
       "      <th>scientist</th>\n",
       "      <th>get</th>\n",
       "      <th>vengeance</th>\n",
       "      <th>data</th>\n",
       "      <th>peoples</th>\n",
       "      <th>behavior</th>\n",
       "      <th>trailer</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>HEALTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047766</td>\n",
       "      <td>0.047766</td>\n",
       "      <td>0.047766</td>\n",
       "      <td>0.017629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047766</td>\n",
       "      <td>0.047766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095532</td>\n",
       "      <td>0.095532</td>\n",
       "      <td>0.047766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>TECH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.091551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091551</td>\n",
       "      <td>0.091551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033789</td>\n",
       "      <td>0.091551</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091551</td>\n",
       "      <td>TV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       news       may  spoilers   science  hundreds  experiments       men  \\\n",
       "0  0.000000  0.057924  0.000000  0.000000  0.000000     0.000000  0.057924   \n",
       "1  0.000000  0.017629  0.000000  0.047766  0.047766     0.047766  0.017629   \n",
       "2  0.091551  0.000000  0.091551  0.000000  0.000000     0.000000  0.000000   \n",
       "\n",
       "   sparking    trying       run   ...        game    rumors  scientist  \\\n",
       "0  0.000000  0.000000  0.000000   ...    0.000000  0.000000   0.000000   \n",
       "1  0.000000  0.047766  0.047766   ...    0.000000  0.000000   0.047766   \n",
       "2  0.091551  0.000000  0.000000   ...    0.091551  0.091551   0.000000   \n",
       "\n",
       "        get  vengeance      data   peoples  behavior   trailer  target  \n",
       "0  0.057924   0.000000  0.000000  0.000000  0.000000  0.000000  HEALTH  \n",
       "1  0.000000   0.000000  0.095532  0.095532  0.047766  0.000000    TECH  \n",
       "2  0.033789   0.091551  0.000000  0.000000  0.000000  0.091551      TV  \n",
       "\n",
       "[3 rows x 36 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = tf_idf.assign(target=['HEALTH', 'TECH', 'TV'])\n",
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's try to classify a sample sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.000020\n",
       "1    0.000025\n",
       "2    0.000020\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_document = 'Data science at Facebook seems fun!'\n",
    "\n",
    "# Computing the probability for each class\n",
    "probas = train_set.target.map(lambda C: prob_document_of_class(new_document, C, train_set))\n",
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TECH'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class = train_set.target.iloc[probas.argmax()]\n",
    "predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, right prediction! Well, it is a **very** toy example, but things seems to make sense.\n",
    "\n",
    "Countexample: what would happen throwing some random text at the model? The desired results should be... no class predicted at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   NaN\n",
       "1   NaN\n",
       "2   NaN\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_document = 'This is a very generic text, so I try to say nothing'\n",
    "probas = train_set.target.map(lambda C: prob_document_of_class(new_document, C, train_set))\n",
    "probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](https://media.apnarm.net.au/media/images/2016/03/21/babymeme_620x310-dsghuu4e57q2783lwl2_ct300x300.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
