{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction\n",
    "\n",
    "We have seen so far that the key approach for handling text data is transforming it into a vectorized feature space. While this is pretty easy to do with tools such as `sklearn`, we usually end up with a **very** high-dimensional feature space, which is difficult to interpret and can lead a classifier to overfit, especially a classifier with high variance (remember the Bias-Variance tradeoff?).\n",
    "\n",
    "For this reason, a common step when processing text data is **dimensionality reduction**. There are a couple of well-known algorithms that transform the data in the high-dimensional space to a space of fewer dimensions. The most widely known are Principal Component Analysis and Singular Value Decomposition. `sklearn` offers both of them.\n",
    "\n",
    "Yet, in this unit we will focus on a different approach to reduce dimensionality, sometime called *feature selection*. We will try to understand which are the most important features (in our case, words) for discriminating the category of our documents. This is a more manual procedure, but much more interpretable that *blackbox-ish* approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# As always, start with some imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already got to know the News Aggregator dataset during the 2nd Learning Unit. Let's load it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('text-in-practice/data/uci-news-aggregator.csv')\n",
    "df = df[['TITLE', 'CATEGORY']]\n",
    "df.columns = ['title', 'category']\n",
    "\n",
    "categories = {\n",
    "    'b': 'business',\n",
    "    't': 'tech',\n",
    "    'e': 'entertainment',\n",
    "    'm': 'health'\n",
    "}\n",
    "\n",
    "df.category.replace(categories, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the validation set along this chapter, since it's smaller and we are pretty confident that it has the same distribution of the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entertainment    30353\n",
       "business         23414\n",
       "tech             21693\n",
       "health            9024\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, validation_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "validation_df.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's time to apply vectorization again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note that CountVectorizer has a lot of optional parameters, some of which are really interesting...\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=2, max_df=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', vectorizer),\n",
    "    ('tfidf', TfidfTransformer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84484, 18979)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectorize\n",
    "vectorized = text_clf.fit_transform(validation_df.title)\n",
    "vectorized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good, but a 18K-dimensional space is a bit tough to interpret. It would be so better to extract the most important `N` words.\n",
    "\n",
    "A way to do that is to embrace a statistical point of view and analyze how much each word is independent of the target. If a given word is independent of the target, that word is probably not so important to predict the target. If we treat each feature as a stochastic variable, we can test the indipendence of this variable with respect to the target, using tests for indipendences that the good folks in statistics developed decades ago.\n",
    "\n",
    "Since our features are *frequencies* (TF-IDF), the proper test for the situation is the chi-squared test $\\chi^2$. A low chi-squared value for a feature means it's indipendent of the target, which in turn means that the feature is not particularly useful for classification.\n",
    "\n",
    "Luckly for us, `sklearn` supports chi-squared with an amazingly simple interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "# SelectKBest is used to retain only the k most important features, according to the specified metric\n",
    "ch2 = SelectKBest(chi2, k=10)\n",
    "X_train = ch2.fit_transform(vectorized, validation_df.category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which are the 10 features that our chi-squared test considered most important:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple',\n",
       " 'cancer',\n",
       " 'ebola',\n",
       " 'google',\n",
       " 'mers',\n",
       " 'microsoft',\n",
       " 'outbreak',\n",
       " 'samsung',\n",
       " 'study',\n",
       " 'virus']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = text_clf.named_steps['vect']\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "most_important_features = [feature_names[i] for i in ch2.get_support(indices=True)]\n",
    "most_important_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sense, right? We can further convince ourselves by looking how frequent those words are among categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents that contains the word apple\n",
      "----\n",
      "tech             7576\n",
      "business          647\n",
      "entertainment      79\n",
      "health             26\n",
      "Name: category, dtype: int64 \n",
      "\n",
      "Documents that contains the word cancer\n",
      "----\n",
      "health           2225\n",
      "entertainment     200\n",
      "business          167\n",
      "tech                5\n",
      "Name: category, dtype: int64 \n",
      "\n",
      "Documents that contains the word ebola\n",
      "----\n",
      "health           2986\n",
      "business            2\n",
      "entertainment       1\n",
      "Name: category, dtype: int64 \n",
      "\n",
      "Documents that contains the word google\n",
      "----\n",
      "tech             8887\n",
      "business          692\n",
      "health             79\n",
      "entertainment      31\n",
      "Name: category, dtype: int64 \n",
      "\n",
      "Documents that contains the word mers\n",
      "----\n",
      "health           1745\n",
      "entertainment    1123\n",
      "tech              949\n",
      "business          561\n",
      "Name: category, dtype: int64 \n",
      "\n",
      "Documents that contains the word microsoft\n",
      "----\n",
      "tech             5189\n",
      "business          417\n",
      "entertainment       5\n",
      "health              1\n",
      "Name: category, dtype: int64 \n",
      "\n",
      "Documents that contains the word outbreak\n",
      "----\n",
      "health           1242\n",
      "entertainment       3\n",
      "business            3\n",
      "Name: category, dtype: int64 \n",
      "\n",
      "Documents that contains the word samsung\n",
      "----\n",
      "tech             6041\n",
      "business          278\n",
      "health             91\n",
      "entertainment       7\n",
      "Name: category, dtype: int64 \n",
      "\n",
      "Documents that contains the word study\n",
      "----\n",
      "health           2764\n",
      "tech              705\n",
      "business          273\n",
      "entertainment      40\n",
      "Name: category, dtype: int64 \n",
      "\n",
      "Documents that contains the word virus\n",
      "----\n",
      "health           1706\n",
      "entertainment      89\n",
      "tech               81\n",
      "business           58\n",
      "Name: category, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for feature in most_important_features:\n",
    "    print('Documents that contains the word %s' % feature)\n",
    "    print('----')\n",
    "    docs = train_df.title.str.lower().str.contains(feature)\n",
    "    print(train_df.category[docs].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite its simplicity, this approach can be really useful to understand which pattern our classifier it's going to capture. Can you spot one of the problems with `chi2`?\n",
    "\n",
    "*Hint*: it considers only one feature (word) at a time. Sometimes, a combination of unimportant features turns into a very discriminant feature."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nteract": {
   "version": "0.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
