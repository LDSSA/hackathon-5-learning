{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first 2 learning units, we saw the so-called **Bag of Words** vectorization of text. Text is transformed in vectors, constituted by counts or frequencies of words. This representation has the advantage of being simple and lasted for a very long time.\n",
    "\n",
    "A possible improvement to this idea is, instead of considering single words, to consider sequences of them. So, you can create features for 2 consecutives words, or even 3. You can even consider sequences of letters, or phonems. This approach is called **n-grams** reprensentation, and it could be very effective as well. Since we are not going to cover it in this material, you can find a very interesting explanation [here](https://www.youtube.com/watch?v=s3kKlUBa3b0&index=12&list=PL6397E4B26D00A269).\n",
    "\n",
    "An important breakthrough in text representation has been made during recent past by using deep learning. The family of algorithms called **word embedding** - the most famous being `word2vec` - is based on training neural network to automatically learn the more informative representation for text data. The networks are trained to produce vectors that embed semantic information extracted from text. For example,  words with similar meaning produces vectors that are close to one another in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paul\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# GenSim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train those kind of models is **very** computationally expensive. The good news is that, once trained by those guys with the big computers, we can happily use the trained models to project text to the feature space. \n",
    "\n",
    "Thankfully, we can go [here](http://nlpserver2.inf.ufrgs.br/alexandres/vectors/lexvec.enwiki%2bnewscrawl.300d.W.pos.vectors.gz) and download a LexVec word embedding model (similar to word2vec and [GloVe](https://nlp.stanford.edu/projects/glove/)) pre-trained on Wikipedia.\n",
    "\n",
    "Small note: the models have to be trained on text of the same language. So, one of the problems with such an approach is that most of tools are only available for English language.\n",
    "\n",
    "Another small note: be careful, this will load a **pretty big** object in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./lexvec.enwiki+newscrawl.300d.W.pos.vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said earlier, we have some cool properties in this vector space. We can treat words as numbers. What do you expect from `woman + king - man`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6212160587310791),\n",
       " ('monarch', 0.5939740538597107),\n",
       " ('prince', 0.5655953884124756),\n",
       " ('throne', 0.5191947817802429),\n",
       " ('princess', 0.5010462403297424),\n",
       " ('emperor', 0.4941236674785614),\n",
       " ('consort', 0.474475622177124),\n",
       " ('empress', 0.4712255299091339),\n",
       " ('regent', 0.46504777669906616),\n",
       " ('betrothed', 0.4598500430583954)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that was nice. Let's see how things work with real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID            int64\n",
       "TITLE        object\n",
       "URL          object\n",
       "PUBLISHER    object\n",
       "CATEGORY     object\n",
       "STORY        object\n",
       "HOSTNAME     object\n",
       "TIMESTAMP     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../hackathon-5-learning/02-text-in-practice/data/uci-news-aggregator.csv')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already used this dataset in the last notebook but to give you some context, this dataset is made up of a different news headlines and their respective category. Let's look at some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fed official says weak data caused by weather,...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US open: Stocks fall after Fed official hints ...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fed risks falling 'behind the curve', Charles ...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fed's Plosser: Nasty Weather Has Curbed Job Gr...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title category\n",
       "0  Fed official says weak data caused by weather,...        b\n",
       "1  Fed's Charles Plosser sees high bar for change...        b\n",
       "2  US open: Stocks fall after Fed official hints ...        b\n",
       "3  Fed risks falling 'behind the curve', Charles ...        b\n",
       "4  Fed's Plosser: Nasty Weather Has Curbed Job Gr...        b"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['TITLE', 'CATEGORY']]\n",
    "df.columns = ['title', 'category']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plit our data into train and validation set so we can benchmark the final model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "_, data = train_test_split(df, test_size=0.3, random_state=42)  # subsampling to help performance\n",
    "train_df, validation_df = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the dataset loaded and set up we get into the interesting part, transforming the document from a variable length sequence of words into a fixed length vector representation!\n",
    "\n",
    "As we have talked about in previous hackathons, categorical features are generally converted not in an integer but in a onehot encoding of the category. That is, giving a small example:\n",
    "\n",
    "If the possible values of a given feature \"gender\" would be:\n",
    "\n",
    "[\"Male\", \"Female\", \"Unknown\", \"Attack Helicopter\"]\n",
    "\n",
    "The entry with the feature \"Male\" would be represented as:\n",
    "\n",
    "(1, 0, 0, 0)\n",
    "\n",
    "This procedure is important as we don't impose any information about the problem, if we would convert the values into either 1, 2, 3 or 4 we would be giving additional information as we could lead our classifier to believe that unkown and female are similar concepts since 2 and 3 are close on the line of real numbers.\n",
    "\n",
    "Here is where word vectors come in handy! Word vectors help us find a representation in a continuous space (latent space) where similar words are close in space!\n",
    "\n",
    "The previous example (woman + (king - queen)) aimed to show the structure of this space.\n",
    "\n",
    "The model we loaded earlier keeps a dictionary of words and their vector representation in this space (model.wv). Looking at the word vectors of a concrete example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vector dimension:  (300,)\n",
      "Similarity between King and Queen:  3.93262\n",
      "Similarity between Kind and Peasant:  0.574412\n"
     ]
    }
   ],
   "source": [
    "# Load the word vector for the word \"car\"\n",
    "king_vector = model.wv['king']\n",
    "\n",
    "print(\"Word vector dimension: \", king_vector.shape)\n",
    "\n",
    "# Load the word vector for the word \"bus\" and \"cat\"\n",
    "queen_vector = model.wv['queen']\n",
    "peasant_vector = model.wv['peasant']\n",
    "\n",
    "print(\"Similarity between King and Queen: \", np.dot(king_vector, queen_vector))\n",
    "print(\"Similarity between Kind and Peasant: \", np.dot(king_vector, peasant_vector))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we used the dot product as a means of calculating how similar two vectors are, we can see that the model assigns vectors which are similar for \"king\" and \"queen\" but separates them in space from the concept of \"peasant\". \n",
    "\n",
    "Small side note: While it is curious to observe a political agenda in a word vector embedding model, this is an example of possible human bias in the input data and alerts to the dangers of giving our models biased data. In the end we are all humans, being peasant or king!     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document vector representation\n",
    "\n",
    "Now that we looked at our first word vectors, we are ready to move on to convert our documents into this space!\n",
    "\n",
    "We start by splitting our sentences in words (tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UPDATE',\n",
       " 'China',\n",
       " 'factories',\n",
       " 'struggle',\n",
       " 'adds',\n",
       " 'to',\n",
       " 'expectations',\n",
       " 'for',\n",
       " 'stimulus']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract document tokens\n",
    "\n",
    "tokenizer = tfidf.build_tokenizer()\n",
    "\n",
    "documents = train_df\n",
    "tokens = [[word for word in tokenizer(document)] for document in documents['title']]\n",
    "\n",
    "tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the sentence sepparated in tokens we can easily convert the tokens into the word vector space using the Word2Vec model's word vector dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Convert each token (word) into vector representation\n",
    "vectors = [np.array([model.wv[token] if token in model.wv else model.wv['the'] for token in sentence]) for sentence in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a single sentence, we can see that the sentence has become an array of vectors, a matrix. Why matrix you ask? \n",
    "\n",
    "It becomes a matrix when we convert the tokens in the sentence to vecctors. Looking at the shape of a sentence we can see that it is of the dimension (9 x 300), where 9 is the number of words in the sentence and 300 the dimension of the word vector space. This means in one sentence we have 9 word vectors where each has a dimension of 300 entries. Sounds about right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each document (sentence), we have now have an array of vectors. One vector for each word. Argh, that's a lot of numbers! But don't dispair, we can \"summarize\" each document!\n",
    "\n",
    "There are far smarter ways to do that, but to keep things simple, we can take the mean of all the words. We end up with a fixed size vector for each document!:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101380, 300)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average word vectors to get document representation\n",
    "vectorized = np.array([document.mean(axis=0) for document in vectors])\n",
    "vectorized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a single entry we can see that the dimension of the sentence representation is 300, this is natural since we averaged the representation of all the vectors in the sentence! \n",
    "\n",
    "Like everything in life, there is no free lunch! By reducing the array of vectors to a fixed sized representtion of 300 by averaging the word vectors we loose some information! Nevertheless this averaged representation captures the average location of words in the word vector space! \n",
    "\n",
    "For example, if the sentence is made up of words in the general field of finance, the average of these word vectors would be somewhere in the finance region of the word vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 0.5662599802017212),\n",
       " ('the', 0.5639806985855103),\n",
       " ('expectations', 0.5170621871948242),\n",
       " ('demand', 0.48753446340560913),\n",
       " ('stimulus', 0.4867039918899536),\n",
       " ('its', 0.4852517545223236),\n",
       " ('sluggish', 0.4840191900730133),\n",
       " ('worries', 0.48069673776626587),\n",
       " ('expect', 0.47866320610046387),\n",
       " ('slowdown', 0.4760039746761322)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_vector(vectorized[0], topn=10, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the words around the region of the sentence vector, this vector does not seem to capture a lot of information! \n",
    "\n",
    "Let's try to fix this by weighting the average by the term frequencies so these generic words are ignored!\n",
    "\n",
    "We start by creating a dictionary of the term frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(preprocessor=lambda x: x)\n",
    "tfidf.fit(train_df['title'])\n",
    "\n",
    "tfidf_dictionary = {w: tfidf.idf_[i] for w, i in tfidf.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a vector with each sentences token weights and average the word vectors using this weighting vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "weights = [[tfidf_dictionary[token] for token in sequence] for sequence in tokens]\n",
    "weighted_vectors = np.array([np.average(vectors[i], weights=weights[i], axis=0) for i in range(0, len(documents))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('expectations', 0.5419510006904602),\n",
       " ('economy', 0.5358553528785706),\n",
       " ('stimulus', 0.5331944823265076),\n",
       " ('slowdown', 0.5285102725028992),\n",
       " ('downturn', 0.524492621421814),\n",
       " ('sluggish', 0.5207642912864685),\n",
       " ('demand', 0.520745038986206),\n",
       " ('worries', 0.517106294631958),\n",
       " ('jobs', 0.5140954256057739),\n",
       " ('growth', 0.5140053033828735)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_vector(weighted_vectors[0], topn=10, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correcting for the term frequencies we were able to get a much better document representation! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "source": [
    "As you can see, this representation captures some semantical structure of the sentence. In some problems, this can produce very discriminative features.\n",
    "\n",
    "This is a fairly basic approach that can be further improved in many ways, from the model used to extract the embeddings (vectors) to the strategy used to combine the word vectors into the sentence vector."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nteract": {
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
